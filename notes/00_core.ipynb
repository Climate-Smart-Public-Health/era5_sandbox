{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> This is a core library for the ERA5 dataset pipeline. It defines\n",
    "a few helpful functions such as an API tester to test your API key and connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os\n",
    "import cdsapi\n",
    "import hydra\n",
    "import json\n",
    "import tempfile\n",
    "import argparse\n",
    "import geopandas as gpd\n",
    "from pydrive2.auth import GoogleAuth\n",
    "from pydrive2.drive import GoogleDrive\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from pyprojroot import here\n",
    "from importlib import import_module\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities\n",
    "\n",
    "Some utilities are provided to help you with the ERA5 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def describe(\n",
    "    cfg: DictConfig=None,  # Configuration file\n",
    "    )-> None:\n",
    "    \"Describe the configuration file used by Hydra for the pipeline\"\n",
    "    \n",
    "    if cfg is None:\n",
    "        print(\"No configuration file provided. Generating default configuration file.\")\n",
    "        cfg = OmegaConf.create()\n",
    "        \n",
    "    print(\"This package fetches ERA5 data. The following is the config file used by Hydra for the pipeline:\\n\")\n",
    "    print(OmegaConf.to_yaml(cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _expand_path(\n",
    "        path: str   # Path on user's machine\n",
    "        )->   str:  # Expanded path\n",
    "    \"Expand the path on the user's machine for cross compatibility\"\n",
    "\n",
    "    # Expand ~ to the user's home directory\n",
    "    path = os.path.expanduser(path)\n",
    "    # Expand environment variables\n",
    "    path = os.path.expandvars(path)\n",
    "    # Convert to absolute path\n",
    "    path = os.path.abspath(path)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _get_callable(func_path):\n",
    "    \"\"\"Dynamically import a callable from a string path.\"\"\"\n",
    "    module_name, func_name = func_path.rsplit(\".\", 1)\n",
    "    module = import_module(module_name)\n",
    "    return getattr(module, func_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "def _create_directory_structure(\n",
    "        base_path: str,  # The base directory where the structure will be created\n",
    "        structure: dict  # A dictionary representing the directory structure\n",
    "    )->None:\n",
    "    \"\"\"\n",
    "    Recursively creates a directory structure from a dictionary.\n",
    "\n",
    "    Args:\n",
    "        base_path (str): The base directory where the structure will be created.\n",
    "        structure (dict): A dictionary representing the directory structure.\n",
    "    \"\"\"\n",
    "    for folder, substructure in structure.items():\n",
    "        # Create the current directory\n",
    "        current_path = os.path.join(base_path, folder)\n",
    "        os.makedirs(current_path, exist_ok=True)\n",
    "        \n",
    "        # Recursively create subdirectories if substructure is a dictionary\n",
    "        if isinstance(substructure, dict):\n",
    "            _create_directory_structure(current_path, substructure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Class for Authenticating Google Drive\n",
    "\n",
    "We're going to use a class to authenticate and interact with google drive. The goal is to have a simple interface to fetch the healthshed files dynamically from google drive in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class GoogleDriver:\n",
    "    \"\"\"\n",
    "    A class to handle Google Drive authentication and file management.\n",
    "    This class uses the PyDrive2 library to authenticate with Google Drive using a service account.\n",
    "    \n",
    "    It provides three methods: authenticating the account, getting the drive object, and downloading the healthshed files for madagascar.\n",
    "    \"\"\"\n",
    "    def __init__(self, json_key_path=None):\n",
    "        self.json_key_path = json_key_path or os.getenv(\"GOOGLE_DRIVE_AUTH_JSON\")\n",
    "        if not self.json_key_path or not os.path.isfile(self.json_key_path):\n",
    "            raise FileNotFoundError(f\"Service account key file not found: {self.json_key_path}\")\n",
    "        self.drive = self._authenticate()\n",
    "\n",
    "    def _authenticate(self):\n",
    "\n",
    "        settings = {\n",
    "            \"client_config_backend\": \"service\",\n",
    "            \"service_config\": {\n",
    "                \"client_json_file_path\": self.json_key_path\n",
    "            }\n",
    "        }\n",
    "        gauth = GoogleAuth(settings=settings)\n",
    "\n",
    "        gauth.ServiceAuth()\n",
    "        \n",
    "        return GoogleDrive(gauth)\n",
    "\n",
    "    def get_drive(self):\n",
    "        return self.drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how we use it. The credentials for the data-pipeline service account are\n",
    "available in the sandbox folder, and the path to said folder is set in the config:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydra import initialize, compose\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "# unfortunately, we have to use the initialize function to load the config file\n",
    "# this is because the @hydra decorator does not work with Notebooks very well\n",
    "# this is a known issue with Hydra: https://gist.github.com/bdsaglam/586704a98336a0cf0a65a6e7c247d248\n",
    "# \n",
    "# just use the relative path from the notebook to the config dir\n",
    "with initialize(version_base=None, config_path=\"../conf\"):\n",
    "    cfg = compose(config_name='config.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = GoogleDriver(json_key_path=here() / cfg.GOOGLE_DRIVE_AUTH_JSON.path)\n",
    "drive = auth.get_drive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how we might check that the healthsheds are accessible in the drive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "healthsheds2022.zip - application/zip\n"
     ]
    }
   ],
   "source": [
    "# we're using the madagascar healthshed folder as an example\n",
    "folder_id = cfg.geographies.madagascar.healthsheds\n",
    "folder_name = \"healthsheds2022.zip\"\n",
    "file_list = drive.ListFile({'q': f\" title='{folder_name}' and trashed = false \"}).GetList()\n",
    "\n",
    "for file in file_list:\n",
    "    print(f\"{file['title']} - {file['mimeType']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That being said, we can read in  the healthsheds into geopandas by downloading them to a temp directory. The healthsheds must be a zipped shapefiles package with the files at the root of the zip directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tempfile.TemporaryDirectory() as temp_dir:\n",
    "    # Create a temporary directory to store the downloaded file\n",
    "    zip_path = os.path.join(temp_dir, folder_name)\n",
    "\n",
    "    # Download file from Google Drive\n",
    "    file_obj = drive.CreateFile({'id': file_list[0]['id']})\n",
    "    file_obj.GetContentFile(zip_path)\n",
    "\n",
    "    # Read shapefile directly from ZIP\n",
    "    gdf = gpd.read_file(f\"zip://{zip_path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That works! So now we can patch the class to include this workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastcore.basics import patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@patch\n",
    "def read_healthsheds(self:GoogleDriver, healthshed_zip_name):\n",
    "\n",
    "    file_list = self.drive.ListFile({'q': f\" title='{healthshed_zip_name}' and trashed = false \"}).GetList()\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        # Create a temporary directory to store the downloaded file\n",
    "        zip_path = os.path.join(temp_dir, healthshed_zip_name)\n",
    "\n",
    "        # Download file from Google Drive\n",
    "        file_obj = self.drive.CreateFile({'id': file_list[0]['id']})\n",
    "        file_obj.GetContentFile(zip_path)\n",
    "\n",
    "        # Read shapefile directly from ZIP\n",
    "        gdf = gpd.read_file(f\"zip://{zip_path}\")\n",
    "\n",
    "        # we need to ensure that the healthsheds only contain valid polygons\n",
    "        gdf = gdf[gdf.geometry.notnull()]\n",
    "        gdf.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        return gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to check that it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fs_pop</th>\n",
       "      <th>n_uid</th>\n",
       "      <th>n_instat</th>\n",
       "      <th>n_comp</th>\n",
       "      <th>n_shape</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2766.000000</td>\n",
       "      <td>2766.000000</td>\n",
       "      <td>2766.000000</td>\n",
       "      <td>2766.000000</td>\n",
       "      <td>2766.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>10493.058930</td>\n",
       "      <td>7.480116</td>\n",
       "      <td>6.318149</td>\n",
       "      <td>1.010484</td>\n",
       "      <td>1.036515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>12127.817529</td>\n",
       "      <td>7.263235</td>\n",
       "      <td>4.939271</td>\n",
       "      <td>0.112019</td>\n",
       "      <td>0.393120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4344.750000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7417.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>12531.250000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>194782.000000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>15.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              fs_pop        n_uid     n_instat       n_comp      n_shape\n",
       "count    2766.000000  2766.000000  2766.000000  2766.000000  2766.000000\n",
       "mean    10493.058930     7.480116     6.318149     1.010484     1.036515\n",
       "std     12127.817529     7.263235     4.939271     0.112019     0.393120\n",
       "min         0.000000     1.000000     1.000000     1.000000     1.000000\n",
       "25%      4344.750000     4.000000     3.000000     1.000000     1.000000\n",
       "50%      7417.000000     6.000000     5.000000     1.000000     1.000000\n",
       "75%     12531.250000     9.000000     8.000000     1.000000     1.000000\n",
       "max    194782.000000   104.000000    62.000000     3.000000    15.000000"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver = GoogleDriver(json_key_path=here() / cfg.GOOGLE_DRIVE_AUTH_JSON.path)\n",
    "drive = driver.get_drive()\n",
    "healthsheds = driver.read_healthsheds(\"healthsheds2022.zip\")\n",
    "\n",
    "healthsheds.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests and Main\n",
    "\n",
    "In `nbdev`, our tests are embedded in the notebook. Whenever you export the notebook, all the cells that are specified to run are run, and hence, the tests are executed. The tests are also exported. This is a great way to ensure that your documentation is always up-to-date. For this module, we're using the `testAPI()` function as our main test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def testAPI(\n",
    "    cfg: DictConfig=None,\n",
    "    dataset:str=\"reanalysis-era5-pressure-levels\"\n",
    "    )-> bool:    \n",
    "    \n",
    "    # parse config\n",
    "    testing=cfg.development_mode\n",
    "    output_path=here(\"data\") / \"testing\"\n",
    "\n",
    "    print(OmegaConf.to_yaml(cfg))\n",
    "\n",
    "    try:\n",
    "        client = cdsapi.Client()\n",
    "\n",
    "        # build request\n",
    "        request = {\n",
    "            'product_type': ['reanalysis'],\n",
    "            'variable': ['geopotential'],\n",
    "            'year': ['2024'],\n",
    "            'month': ['03'],\n",
    "            'day': ['01'],\n",
    "            'time': ['13:00'],\n",
    "            'pressure_level': ['1000'],\n",
    "            'data_format': 'grib',\n",
    "        }\n",
    "\n",
    "        target = output_path / 'test_download.grib'\n",
    "        \n",
    "        print(\"Testing API connection by downloading a dummy dataset to {}...\".format(output_path))\n",
    "\n",
    "        client.retrieve(dataset, request, target)\n",
    "\n",
    "        if not testing:\n",
    "            os.remove(target)\n",
    "        \n",
    "        print(\"API connection test successful.\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"API connection test failed.\")\n",
    "        print(\"Did you set up your API key with CDS? If not, please visit https://cds.climate.copernicus.eu/how-to-api#install-the-cds-api-client\")\n",
    "        print(\"Error: {}\".format(e))\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this API tester tool works with Hydra configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This package fetches ERA5 data. The following is the config file used by Hydra for the pipeline:\n",
      "\n",
      "query:\n",
      "  product_type: reanalysis\n",
      "  variable:\n",
      "  - 2m_dewpoint_temperature\n",
      "  - 2m_temperature\n",
      "  - skin_temperature\n",
      "  - total_precipitation\n",
      "  year:\n",
      "  - 2010\n",
      "  - 2011\n",
      "  month:\n",
      "  - 1\n",
      "  - 2\n",
      "  - 3\n",
      "  - 4\n",
      "  - 5\n",
      "  - 6\n",
      "  - 7\n",
      "  - 8\n",
      "  - 9\n",
      "  - 10\n",
      "  - 11\n",
      "  - 12\n",
      "  day:\n",
      "  - 1\n",
      "  - 2\n",
      "  - 3\n",
      "  - 4\n",
      "  - 5\n",
      "  - 6\n",
      "  - 7\n",
      "  - 8\n",
      "  - 9\n",
      "  - 10\n",
      "  - 11\n",
      "  - 12\n",
      "  - 13\n",
      "  - 14\n",
      "  - 15\n",
      "  - 16\n",
      "  - 17\n",
      "  - 18\n",
      "  - 19\n",
      "  - 20\n",
      "  - 21\n",
      "  - 22\n",
      "  - 23\n",
      "  - 24\n",
      "  - 25\n",
      "  - 26\n",
      "  - 27\n",
      "  - 28\n",
      "  - 29\n",
      "  - 30\n",
      "  - 31\n",
      "  time:\n",
      "  - 0\n",
      "  - 1\n",
      "  - 2\n",
      "  - 3\n",
      "  - 4\n",
      "  - 5\n",
      "  - 6\n",
      "  - 7\n",
      "  - 8\n",
      "  - 9\n",
      "  - 10\n",
      "  - 11\n",
      "  - 12\n",
      "  - 13\n",
      "  - 14\n",
      "  - 15\n",
      "  - 16\n",
      "  - 17\n",
      "  - 18\n",
      "  - 19\n",
      "  - 20\n",
      "  - 21\n",
      "  - 22\n",
      "  - 23\n",
      "  area:\n",
      "  - 0\n",
      "  - 360\n",
      "  - -90\n",
      "  - 90\n",
      "  data_format: netcdf\n",
      "  download_format: unarchived\n",
      "datapaths:\n",
      "  input: null\n",
      "  output: null\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from hydra import initialize, compose\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "# unfortunately, we have to use the initialize function to load the config file\n",
    "# this is because the @hydra decorator does not work with Notebooks very well\n",
    "# this is a known issue with Hydra: https://gist.github.com/bdsaglam/586704a98336a0cf0a65a6e7c247d248\n",
    "# \n",
    "# just use the relative path from the notebook to the config dir\n",
    "with initialize(version_base=None, config_path=\"../conf\"):\n",
    "    cfg = compose(config_name='config.yaml')\n",
    "\n",
    "describe(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the Main Function\n",
    "\n",
    "Important: using `__main__` in nbdev and Hydra is a little bit tricky. We need to define the main function in the module ONLY ONCE and then when we export the notebook to script, we need to add the `nbdev.imports.IN_NOTEBOOK` variable. This way, the main function will only be executed when we run the notebook and not when we import the module.\n",
    "\n",
    "```python\n",
    "from nbdev.imports import IN_NOTEBOOK\n",
    "```\n",
    "\n",
    "You'll see this listed throughout the notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@hydra.main(version_base=None, config_path=\"../../conf\", config_name=\"config\")\n",
    "def main(cfg: DictConfig) -> None:\n",
    "\n",
    "    # Create the directory structure\n",
    "    _create_directory_structure(here() / \"data\", cfg.datapaths)\n",
    "\n",
    "    # test the api\n",
    "    testAPI(cfg=cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#| eval: false\n",
    "try: from nbdev.imports import IN_NOTEBOOK\n",
    "except: IN_NOTEBOOK=False\n",
    "\n",
    "if __name__ == \"__main__\" and not IN_NOTEBOOK:\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
